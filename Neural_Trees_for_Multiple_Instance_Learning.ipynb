{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/mgwoozdz/Neural-Trees-for-Multiple-Instance-Learning/blob/main/Neural_Trees_for_Multiple_Instance_Learning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataloader.py\n",
    "\n",
    "\"\"\"Pytorch dataset object that loads MNIST dataset as bags.\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.utils.data as data_utils\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "\n",
    "class MnistBags(data_utils.Dataset):\n",
    "    def __init__(self, target_number=9, mean_bag_length=10, var_bag_length=2, num_bag=250, seed=1, train=True):\n",
    "        self.target_number = target_number\n",
    "        self.mean_bag_length = mean_bag_length\n",
    "        self.var_bag_length = var_bag_length\n",
    "        self.num_bag = num_bag\n",
    "        self.train = train\n",
    "\n",
    "        self.r = np.random.RandomState(seed)\n",
    "\n",
    "        self.num_in_train = 60000\n",
    "        self.num_in_test = 10000\n",
    "\n",
    "        if self.train:\n",
    "            self.train_bags_list, self.train_labels_list = self._create_bags()\n",
    "        else:\n",
    "            self.test_bags_list, self.test_labels_list = self._create_bags()\n",
    "\n",
    "    def _create_bags(self):\n",
    "        if self.train:\n",
    "            loader = data_utils.DataLoader(datasets.MNIST('../datasets',\n",
    "                                                          train=True,\n",
    "                                                          download=True,\n",
    "                                                          transform=transforms.Compose([\n",
    "                                                              transforms.ToTensor(),\n",
    "                                                              transforms.Normalize((0.1307,), (0.3081,))])),\n",
    "                                           batch_size=self.num_in_train,\n",
    "                                           shuffle=False)\n",
    "        else:\n",
    "            loader = data_utils.DataLoader(datasets.MNIST('../datasets',\n",
    "                                                          train=False,\n",
    "                                                          download=True,\n",
    "                                                          transform=transforms.Compose([\n",
    "                                                              transforms.ToTensor(),\n",
    "                                                              transforms.Normalize((0.1307,), (0.3081,))])),\n",
    "                                           batch_size=self.num_in_test,\n",
    "                                           shuffle=False)\n",
    "\n",
    "        for (batch_data, batch_labels) in loader:\n",
    "            all_imgs = batch_data\n",
    "            all_labels = batch_labels\n",
    "\n",
    "        bags_list = []\n",
    "        labels_list = []\n",
    "\n",
    "        for i in range(self.num_bag):\n",
    "            bag_length = np.int(self.r.normal(self.mean_bag_length, self.var_bag_length, 1))\n",
    "            if bag_length < 1:\n",
    "                bag_length = 1\n",
    "\n",
    "            if self.train:\n",
    "                indices = torch.LongTensor(self.r.randint(0, self.num_in_train, bag_length))\n",
    "            else:\n",
    "                indices = torch.LongTensor(self.r.randint(0, self.num_in_test, bag_length))\n",
    "\n",
    "            labels_in_bag = all_labels[indices]\n",
    "            labels_in_bag = labels_in_bag == self.target_number\n",
    "\n",
    "            bags_list.append(all_imgs[indices])\n",
    "            labels_list.append(labels_in_bag)\n",
    "\n",
    "        return bags_list, labels_list\n",
    "\n",
    "    def __len__(self):\n",
    "        if self.train:\n",
    "            return len(self.train_labels_list)\n",
    "        else:\n",
    "            return len(self.test_labels_list)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        if self.train:\n",
    "            bag = self.train_bags_list[index]\n",
    "            label = [max(self.train_labels_list[index]), self.train_labels_list[index]]\n",
    "        else:\n",
    "            bag = self.test_bags_list[index]\n",
    "            label = [max(self.test_labels_list[index]), self.test_labels_list[index]]\n",
    "\n",
    "        return bag, label\n",
    "\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "\n",
    "#     train_loader = data_utils.DataLoader(MnistBags(target_number=9,\n",
    "#                                                    mean_bag_length=10,\n",
    "#                                                    var_bag_length=2,\n",
    "#                                                    num_bag=100,\n",
    "#                                                    seed=1,\n",
    "#                                                    train=True),\n",
    "#                                          batch_size=1,\n",
    "#                                          shuffle=True)\n",
    "\n",
    "#     test_loader = data_utils.DataLoader(MnistBags(target_number=9,\n",
    "#                                                   mean_bag_length=10,\n",
    "#                                                   var_bag_length=2,\n",
    "#                                                   num_bag=100,\n",
    "#                                                   seed=1,\n",
    "#                                                   train=False),\n",
    "#                                         batch_size=1,\n",
    "#                                         shuffle=False)\n",
    "\n",
    "#     len_bag_list_train = []\n",
    "#     mnist_bags_train = 0\n",
    "#     for batch_idx, (bag, label) in enumerate(train_loader):\n",
    "#         len_bag_list_train.append(int(bag.squeeze(0).size()[0]))\n",
    "#         mnist_bags_train += label[0].numpy()[0]\n",
    "#     print('Number positive train bags: {}/{}\\n'\n",
    "#           'Number of instances per bag, mean: {}, max: {}, min {}\\n'.format(\n",
    "#         mnist_bags_train, len(train_loader),\n",
    "#         np.mean(len_bag_list_train), np.max(len_bag_list_train), np.min(len_bag_list_train)))\n",
    "\n",
    "#     len_bag_list_test = []\n",
    "#     mnist_bags_test = 0\n",
    "#     for batch_idx, (bag, label) in enumerate(test_loader):\n",
    "#         len_bag_list_test.append(int(bag.squeeze(0).size()[0]))\n",
    "#         mnist_bags_test += label[0].numpy()[0]\n",
    "#     print('Number positive test bags: {}/{}\\n'\n",
    "#           'Number of instances per bag, mean: {}, max: {}, min {}\\n'.format(\n",
    "#         mnist_bags_test, len(test_loader),\n",
    "#         np.mean(len_bag_list_test), np.max(len_bag_list_test), np.min(len_bag_list_test)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mnist_bags_loader.py\n",
    "\n",
    "\"\"\"Pytorch Dataset object that loads perfectly balanced MNIST dataset in bag form.\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.utils.data as data_utils\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "\n",
    "class MnistBags(data_utils.Dataset):\n",
    "    def __init__(self, target_number=9, mean_bag_length=10, var_bag_length=1, num_bag=1000, seed=7, train=True):\n",
    "        self.target_number = target_number\n",
    "        self.mean_bag_length = mean_bag_length\n",
    "        self.var_bag_length = var_bag_length\n",
    "        self.num_bag = num_bag\n",
    "        self.seed = seed\n",
    "        self.train = train\n",
    "\n",
    "        self.r = np.random.RandomState(seed)\n",
    "\n",
    "        self.num_in_train = 60000\n",
    "        self.num_in_test = 10000\n",
    "\n",
    "        if self.train:\n",
    "            self.train_bags_list, self.train_labels_list = self._form_bags()\n",
    "        else:\n",
    "            self.test_bags_list, self.test_labels_list = self._form_bags()\n",
    "\n",
    "    def _form_bags(self):\n",
    "        if self.train:\n",
    "            train_loader = data_utils.DataLoader(datasets.MNIST('../datasets',\n",
    "                                                                train=True,\n",
    "                                                                download=True,\n",
    "                                                                transform=transforms.Compose([\n",
    "                                                                         transforms.ToTensor(),\n",
    "                                                                         transforms.Normalize((0.1307,), (0.3081,))])),\n",
    "                                                 batch_size=self.num_in_train,\n",
    "                                                 shuffle=False)\n",
    "\n",
    "            bags_list = []\n",
    "            labels_list = []\n",
    "            valid_bags_counter = 0\n",
    "            label_of_last_bag = 0\n",
    "\n",
    "            for batch_data in train_loader:\n",
    "                numbers = batch_data[0]\n",
    "                labels = batch_data[1]\n",
    "\n",
    "            while valid_bags_counter < self.num_bag:\n",
    "                bag_length = np.int(self.r.normal(self.mean_bag_length, self.var_bag_length, 1))\n",
    "                if bag_length < 1:\n",
    "                    bag_length = 1\n",
    "                indices = torch.LongTensor(self.r.randint(0, self.num_in_train, bag_length))\n",
    "                labels_in_bag = labels[indices]\n",
    "\n",
    "                if (self.target_number in labels_in_bag) and (label_of_last_bag == 0):\n",
    "                    labels_in_bag = labels_in_bag >= self.target_number\n",
    "                    labels_list.append(labels_in_bag)\n",
    "                    bags_list.append(numbers[indices])\n",
    "                    label_of_last_bag = 1\n",
    "                    valid_bags_counter += 1\n",
    "                elif label_of_last_bag == 1:\n",
    "                    index_list = []\n",
    "                    bag_length_counter = 0\n",
    "                    while bag_length_counter < bag_length:\n",
    "                        index = torch.LongTensor(self.r.randint(0, self.num_in_train, 1))\n",
    "                        label_temp = labels[index]\n",
    "                        if label_temp.numpy()[0] != self.target_number:\n",
    "                            index_list.append(index)\n",
    "                            bag_length_counter += 1\n",
    "\n",
    "                    index_list = np.array(index_list)\n",
    "                    labels_in_bag = labels[index_list]\n",
    "                    labels_in_bag = labels_in_bag >= self.target_number\n",
    "                    labels_list.append(labels_in_bag)\n",
    "                    bags_list.append(numbers[index_list])\n",
    "                    label_of_last_bag = 0\n",
    "                    valid_bags_counter += 1\n",
    "                else:\n",
    "                    pass\n",
    "\n",
    "        else:\n",
    "            test_loader = data_utils.DataLoader(datasets.MNIST('../datasets',\n",
    "                                                               train=False,\n",
    "                                                               download=True,\n",
    "                                                               transform=transforms.Compose([\n",
    "                                                                    transforms.ToTensor(),\n",
    "                                                                    transforms.Normalize((0.1307,), (0.3081,))])),\n",
    "                                                batch_size=self.num_in_test,\n",
    "                                                shuffle=False)\n",
    "\n",
    "            bags_list = []\n",
    "            labels_list = []\n",
    "            valid_bags_counter = 0\n",
    "            label_of_last_bag = 0\n",
    "\n",
    "            for batch_data in test_loader:\n",
    "                numbers = batch_data[0]\n",
    "                labels = batch_data[1]\n",
    "\n",
    "            while valid_bags_counter < self.num_bag:\n",
    "                bag_length = np.int(self.r.normal(self.mean_bag_length, self.var_bag_length, 1))\n",
    "                if bag_length < 1:\n",
    "                    bag_length = 1\n",
    "                indices = torch.LongTensor(self.r.randint(0, self.num_in_test, bag_length))\n",
    "                labels_in_bag = labels[indices]\n",
    "\n",
    "                if (self.target_number in labels_in_bag) and (label_of_last_bag == 0):\n",
    "                    labels_in_bag = labels_in_bag >= self.target_number\n",
    "                    labels_list.append(labels_in_bag)\n",
    "                    bags_list.append(numbers[indices])\n",
    "                    label_of_last_bag = 1\n",
    "                    valid_bags_counter += 1\n",
    "                elif label_of_last_bag == 1:\n",
    "                    index_list = []\n",
    "                    bag_length_counter = 0\n",
    "                    while bag_length_counter < bag_length:\n",
    "                        index = torch.LongTensor(self.r.randint(0, self.num_in_test, 1))\n",
    "                        label_temp = labels[index]\n",
    "                        if label_temp.numpy()[0] != self.target_number:\n",
    "                            index_list.append(index)\n",
    "                            bag_length_counter += 1\n",
    "\n",
    "                    index_list = np.array(index_list)\n",
    "                    labels_in_bag = labels[index_list]\n",
    "                    labels_in_bag = labels_in_bag >= self.target_number\n",
    "                    labels_list.append(labels_in_bag)\n",
    "                    bags_list.append(numbers[index_list])\n",
    "                    label_of_last_bag = 0\n",
    "                    valid_bags_counter += 1\n",
    "                else:\n",
    "                    pass\n",
    "\n",
    "        return bags_list, labels_list\n",
    "\n",
    "    def __len__(self):\n",
    "        if self.train:\n",
    "            return len(self.train_labels_list)\n",
    "        else:\n",
    "            return len(self.test_labels_list)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        if self.train:\n",
    "            bag = self.train_bags_list[index]\n",
    "            label = [max(self.train_labels_list[index]), self.train_labels_list[index]]\n",
    "        else:\n",
    "            bag = self.test_bags_list[index]\n",
    "            label = [max(self.test_labels_list[index]), self.test_labels_list[index]]\n",
    "\n",
    "        return bag, label\n",
    "\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     to_pil = transforms.Compose([transforms.ToPILImage()])\n",
    "\n",
    "#     kwargs = {}\n",
    "#     batch_size = 1\n",
    "\n",
    "#     train_loader = data_utils.DataLoader(MnistBags(target_number=9,\n",
    "#                                                    mean_bag_length=10,\n",
    "#                                                    var_bag_length=2,\n",
    "#                                                    num_bag=100,\n",
    "#                                                    seed=98,\n",
    "#                                                    train=True),\n",
    "#                                          batch_size=batch_size,\n",
    "#                                          shuffle=False, **kwargs)\n",
    "\n",
    "#     test_loader = data_utils.DataLoader(MnistBags(target_number=9,\n",
    "#                                                   mean_bag_length=10,\n",
    "#                                                   var_bag_length=2,\n",
    "#                                                   num_bag=10,\n",
    "#                                                   seed=98,\n",
    "#                                                   train=False),\n",
    "#                                         batch_size=batch_size,\n",
    "#                                         shuffle=False, **kwargs)\n",
    "\n",
    "#     len_bag_list = []\n",
    "#     mnist_bags_train = 0\n",
    "#     for batch_idx, data in enumerate(train_loader):\n",
    "#         plot_data = data[0].squeeze(0)\n",
    "#         len_bag_list.append(int(plot_data.size()[0]))\n",
    "#         # plot_data = data[0].squeeze(0)\n",
    "#         # num_instances = int(plot_data.size()[0])\n",
    "#         # print(data[1][0])\n",
    "#         # for i in range(num_instances):\n",
    "#         #     plt.subplot(num_instances, 1, i + 1)\n",
    "#         #     to_pil(plot_data[i, :, :, :]).show()\n",
    "#         # plt.show()\n",
    "#         if data[1][0][0] == 1:\n",
    "#             mnist_bags_train += 1\n",
    "#     print('number of bags with 9(s): ', mnist_bags_train)\n",
    "#     print('total number of bags', len(train_loader))\n",
    "#     print(np.mean(len_bag_list), np.min(len_bag_list), np.max(len_bag_list))\n",
    "\n",
    "#     len_bag_list = []\n",
    "#     mnist_bags_test = 0\n",
    "#     for batch_idx, data in enumerate(test_loader):\n",
    "#         plot_data = data[0].squeeze(0)\n",
    "#         len_bag_list.append(int(plot_data.size()[0]))\n",
    "#         if data[1][0][0] == 1:\n",
    "#             mnist_bags_test += 1\n",
    "#     print('number of bags with 9(s): ', mnist_bags_test)\n",
    "#     print('total number of bags', len(test_loader))\n",
    "#     print(np.mean(len_bag_list), np.min(len_bag_list), np.max(len_bag_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.py\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class Attention(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Attention, self).__init__()\n",
    "        self.L = 500\n",
    "        self.D = 128\n",
    "        self.K = 1\n",
    "\n",
    "        self.feature_extractor_part1 = nn.Sequential(\n",
    "            nn.Conv2d(1, 20, kernel_size=5),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, stride=2),\n",
    "            nn.Conv2d(20, 50, kernel_size=5),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, stride=2)\n",
    "        )\n",
    "\n",
    "        self.feature_extractor_part2 = nn.Sequential(\n",
    "            nn.Linear(50 * 4 * 4, self.L),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "\n",
    "        self.attention = nn.Sequential(\n",
    "            nn.Linear(self.L, self.D),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(self.D, self.K)\n",
    "        )\n",
    "\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(self.L*self.K, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.squeeze(0)\n",
    "\n",
    "        H = self.feature_extractor_part1(x)\n",
    "        H = H.view(-1, 50 * 4 * 4)\n",
    "        H = self.feature_extractor_part2(H)  # NxL\n",
    "\n",
    "        A = self.attention(H)  # NxK\n",
    "        A = torch.transpose(A, 1, 0)  # KxN\n",
    "        A = F.softmax(A, dim=1)  # softmax over N\n",
    "\n",
    "        M = torch.mm(A, H)  # KxL\n",
    "\n",
    "        Y_prob = self.classifier(M)\n",
    "        Y_hat = torch.ge(Y_prob, 0.5).float()\n",
    "\n",
    "        return Y_prob, Y_hat, A\n",
    "\n",
    "    # AUXILIARY METHODS\n",
    "    def calculate_classification_error(self, X, Y):\n",
    "        Y = Y.float()\n",
    "        _, Y_hat, _ = self.forward(X)\n",
    "        error = 1. - Y_hat.eq(Y).cpu().float().mean().data\n",
    "\n",
    "        return error, Y_hat\n",
    "\n",
    "    def calculate_objective(self, X, Y):\n",
    "        Y = Y.float()\n",
    "        Y_prob, _, A = self.forward(X)\n",
    "        Y_prob = torch.clamp(Y_prob, min=1e-5, max=1. - 1e-5)\n",
    "        neg_log_likelihood = -1. * (Y * torch.log(Y_prob) + (1. - Y) * torch.log(1. - Y_prob))  # negative log bernoulli\n",
    "\n",
    "        return neg_log_likelihood, A\n",
    "\n",
    "class GatedAttention(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(GatedAttention, self).__init__()\n",
    "        self.L = 500\n",
    "        self.D = 128\n",
    "        self.K = 1\n",
    "\n",
    "        self.feature_extractor_part1 = nn.Sequential(\n",
    "            nn.Conv2d(1, 20, kernel_size=5),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, stride=2),\n",
    "            nn.Conv2d(20, 50, kernel_size=5),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, stride=2)\n",
    "        )\n",
    "\n",
    "        self.feature_extractor_part2 = nn.Sequential(\n",
    "            nn.Linear(50 * 4 * 4, self.L),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "\n",
    "        self.attention_V = nn.Sequential(\n",
    "            nn.Linear(self.L, self.D),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "\n",
    "        self.attention_U = nn.Sequential(\n",
    "            nn.Linear(self.L, self.D),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "        self.attention_weights = nn.Linear(self.D, self.K)\n",
    "\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(self.L*self.K, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.squeeze(0)\n",
    "\n",
    "        H = self.feature_extractor_part1(x)\n",
    "        H = H.view(-1, 50 * 4 * 4)\n",
    "        H = self.feature_extractor_part2(H)  # NxL\n",
    "\n",
    "        A_V = self.attention_V(H)  # NxD\n",
    "        A_U = self.attention_U(H)  # NxD\n",
    "        A = self.attention_weights(A_V * A_U) # element wise multiplication # NxK\n",
    "        A = torch.transpose(A, 1, 0)  # KxN\n",
    "        A = F.softmax(A, dim=1)  # softmax over N\n",
    "\n",
    "        M = torch.mm(A, H)  # KxL\n",
    "\n",
    "        Y_prob = self.classifier(M)\n",
    "        Y_hat = torch.ge(Y_prob, 0.5).float()\n",
    "\n",
    "        return Y_prob, Y_hat, A\n",
    "\n",
    "    # AUXILIARY METHODS\n",
    "    def calculate_classification_error(self, X, Y):\n",
    "        Y = Y.float()\n",
    "        _, Y_hat, _ = self.forward(X)\n",
    "        error = 1. - Y_hat.eq(Y).cpu().float().mean().item()\n",
    "\n",
    "        return error, Y_hat\n",
    "\n",
    "    def calculate_objective(self, X, Y):\n",
    "        Y = Y.float()\n",
    "        Y_prob, _, A = self.forward(X)\n",
    "        Y_prob = torch.clamp(Y_prob, min=1e-5, max=1. - 1e-5)\n",
    "        neg_log_likelihood = -1. * (Y * torch.log(Y_prob) + (1. - Y) * torch.log(1. - Y_prob))  # negative log bernoulli\n",
    "\n",
    "        return neg_log_likelihood, A\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load Train and Test Set\n",
      "Init Model\n"
     ]
    }
   ],
   "source": [
    "# main.py\n",
    "\n",
    "from __future__ import print_function\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import argparse\n",
    "import torch\n",
    "import torch.utils.data as data_utils\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "\n",
    "# from dataloader import MnistBags\n",
    "# from model import Attention, GatedAttention\n",
    "\n",
    "# Training settings\n",
    "# parser = argparse.ArgumentParser(description='PyTorch MNIST bags Example')\n",
    "# parser.add_argument('--epochs', type=int, default=20, metavar='N',\n",
    "#                     help='number of epochs to train (default: 20)')\n",
    "# parser.add_argument('--lr', type=float, default=0.0005, metavar='LR',\n",
    "#                     help='learning rate (default: 0.0005)')\n",
    "# parser.add_argument('--reg', type=float, default=10e-5, metavar='R',\n",
    "#                     help='weight decay')\n",
    "# parser.add_argument('--target_number', type=int, default=9, metavar='T',\n",
    "#                     help='bags have a positive labels if they contain at least one 9')\n",
    "# parser.add_argument('--mean_bag_length', type=int, default=10, metavar='ML',\n",
    "#                     help='average bag length')\n",
    "# parser.add_argument('--var_bag_length', type=int, default=2, metavar='VL',\n",
    "#                     help='variance of bag length')\n",
    "# parser.add_argument('--num_bags_train', type=int, default=200, metavar='NTrain',\n",
    "#                     help='number of bags in training set')\n",
    "# parser.add_argument('--num_bags_test', type=int, default=50, metavar='NTest',\n",
    "#                     help='number of bags in test set')\n",
    "# parser.add_argument('--seed', type=int, default=1, metavar='S',\n",
    "#                     help='random seed (default: 1)')\n",
    "# parser.add_argument('--no-cuda', action='store_true', default=False,\n",
    "#                     help='disables CUDA training')\n",
    "# parser.add_argument('--model', type=str, default='attention', help='Choose b/w attention and gated_attention')\n",
    "\n",
    "# args = parser.parse_args()\n",
    "\n",
    "class Arguments:\n",
    "    \n",
    "    def __init__(self,\n",
    "                 epochs=20,\n",
    "                 lr=10e-4,\n",
    "                 reg=10e-5,\n",
    "                 target_number=9,\n",
    "                 mean_bag_length=10,\n",
    "                 var_bag_length=2,\n",
    "                 num_bags_train=20,\n",
    "                 num_bags_test=50,\n",
    "                 seed=1,\n",
    "                 no_cuda=True,\n",
    "                 model=\"attention\"):\n",
    "        self.epochs=epochs\n",
    "        self.lr=lr\n",
    "        self.reg=reg\n",
    "        self.target_number=target_number\n",
    "        self.mean_bag_length=mean_bag_length\n",
    "        self.var_bag_length=var_bag_length\n",
    "        self.num_bags_train=num_bags_train\n",
    "        self.num_bags_test=num_bags_test\n",
    "        self.seed=seed\n",
    "        self.no_cuda=no_cuda\n",
    "        self.model=model\n",
    "\n",
    "\n",
    "args = Arguments()\n",
    "\n",
    "args.cuda = not args.no_cuda and torch.cuda.is_available()\n",
    "\n",
    "torch.manual_seed(args.seed)\n",
    "if args.cuda:\n",
    "    torch.cuda.manual_seed(args.seed)\n",
    "    print('\\nGPU is ON!')\n",
    "\n",
    "print('Load Train and Test Set')\n",
    "loader_kwargs = {'num_workers': 1, 'pin_memory': True} if args.cuda else {}\n",
    "\n",
    "train_loader = data_utils.DataLoader(MnistBags(target_number=args.target_number,\n",
    "                                               mean_bag_length=args.mean_bag_length,\n",
    "                                               var_bag_length=args.var_bag_length,\n",
    "                                               num_bag=args.num_bags_train,\n",
    "                                               seed=args.seed,\n",
    "                                               train=True),\n",
    "                                     batch_size=1,\n",
    "                                     shuffle=True,\n",
    "                                     **loader_kwargs)\n",
    "\n",
    "test_loader = data_utils.DataLoader(MnistBags(target_number=args.target_number,\n",
    "                                              mean_bag_length=args.mean_bag_length,\n",
    "                                              var_bag_length=args.var_bag_length,\n",
    "                                              num_bag=args.num_bags_test,\n",
    "                                              seed=args.seed,\n",
    "                                              train=False),\n",
    "                                    batch_size=1,\n",
    "                                    shuffle=False,\n",
    "                                    **loader_kwargs)\n",
    "\n",
    "print('Init Model')\n",
    "if args.model=='attention':\n",
    "    model = Attention()\n",
    "elif args.model=='gated_attention':\n",
    "    model = GatedAttention()\n",
    "if args.cuda:\n",
    "    model.cuda()\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=args.lr, betas=(0.9, 0.999), weight_decay=args.reg)\n",
    "\n",
    "\n",
    "def train(epoch):\n",
    "    model.train()\n",
    "    train_loss = 0.\n",
    "    train_error = 0.\n",
    "    for batch_idx, (data, label) in enumerate(train_loader):\n",
    "        bag_label = label[0]\n",
    "        if args.cuda:\n",
    "            data, bag_label = data.cuda(), bag_label.cuda()\n",
    "        data, bag_label = Variable(data), Variable(bag_label)\n",
    "\n",
    "        # reset gradients\n",
    "        optimizer.zero_grad()\n",
    "        # calculate loss and metrics\n",
    "        loss, _ = model.calculate_objective(data, bag_label)\n",
    "        train_loss += loss.data[0]\n",
    "        error, _ = model.calculate_classification_error(data, bag_label)\n",
    "        train_error += error\n",
    "        # backward pass\n",
    "        loss.backward()\n",
    "        # step\n",
    "        optimizer.step()\n",
    "\n",
    "    # calculate loss and error for epoch\n",
    "    train_loss /= len(train_loader)\n",
    "    train_error /= len(train_loader)\n",
    "\n",
    "    print('Epoch: {}, Loss: {:.4f}, Train error: {:.4f}'.format(epoch, train_loss.cpu().numpy()[0], train_error))\n",
    "\n",
    "\n",
    "def test():\n",
    "    model.eval()\n",
    "    test_loss = 0.\n",
    "    test_error = 0.\n",
    "    for batch_idx, (data, label) in enumerate(test_loader):\n",
    "        bag_label = label[0]\n",
    "        instance_labels = label[1]\n",
    "        if args.cuda:\n",
    "            data, bag_label = data.cuda(), bag_label.cuda()\n",
    "        data, bag_label = Variable(data), Variable(bag_label)\n",
    "        loss, attention_weights = model.calculate_objective(data, bag_label)\n",
    "        test_loss += loss.data[0]\n",
    "        error, predicted_label = model.calculate_classification_error(data, bag_label)\n",
    "        test_error += error\n",
    "\n",
    "        if batch_idx < 5:  # plot bag labels and instance labels for first 5 bags\n",
    "            bag_level = (bag_label.cpu().data.numpy()[0], int(predicted_label.cpu().data.numpy()[0][0]))\n",
    "            instance_level = list(zip(instance_labels.numpy()[0].tolist(),\n",
    "                                 np.round(attention_weights.cpu().data.numpy()[0], decimals=3).tolist()))\n",
    "\n",
    "            print('\\nTrue Bag Label, Predicted Bag Label: {}\\n'\n",
    "                  'True Instance Labels, Attention Weights: {}'.format(bag_level, instance_level))\n",
    "\n",
    "    test_error /= len(test_loader)\n",
    "    test_loss /= len(test_loader)\n",
    "\n",
    "    print('\\nTest Set, Loss: {:.4f}, Test error: {:.4f}'.format(test_loss.cpu().numpy()[0], test_error))\n",
    "\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     print('Start Training')\n",
    "#     for epoch in range(1, args.epochs + 1):\n",
    "#         train(epoch)\n",
    "#     print('Start Testing')\n",
    "#     test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start Training\n",
      "Epoch: 1, Loss: 0.7381, Train error: 0.5500\n",
      "Epoch: 2, Loss: 0.6817, Train error: 0.4000\n",
      "Epoch: 3, Loss: 0.6605, Train error: 0.4000\n",
      "Epoch: 4, Loss: 0.5994, Train error: 0.4000\n",
      "Epoch: 5, Loss: 0.7965, Train error: 0.3000\n",
      "Epoch: 6, Loss: 0.5494, Train error: 0.1000\n",
      "Epoch: 7, Loss: 0.3999, Train error: 0.1000\n",
      "Epoch: 8, Loss: 0.2606, Train error: 0.1000\n",
      "Epoch: 9, Loss: 0.1657, Train error: 0.0500\n",
      "Epoch: 10, Loss: 0.0883, Train error: 0.0500\n",
      "Epoch: 11, Loss: 0.0214, Train error: 0.0000\n",
      "Epoch: 12, Loss: 0.1629, Train error: 0.1000\n",
      "Epoch: 13, Loss: 0.4894, Train error: 0.3500\n",
      "Epoch: 14, Loss: 0.3745, Train error: 0.1500\n",
      "Epoch: 15, Loss: 0.1238, Train error: 0.0000\n",
      "Epoch: 16, Loss: 0.0393, Train error: 0.0000\n",
      "Epoch: 17, Loss: 0.0146, Train error: 0.0000\n",
      "Epoch: 18, Loss: 0.0572, Train error: 0.0500\n",
      "Epoch: 19, Loss: 0.0235, Train error: 0.0000\n",
      "Epoch: 20, Loss: 0.0033, Train error: 0.0000\n",
      "Start Testing\n",
      "\n",
      "True Bag Label, Predicted Bag Label: (True, 0)\n",
      "True Instance Labels, Attention Weights: [(False, 0.0), (True, 0.0), (False, 0.0), (False, 1.0), (False, 0.0), (False, 0.0), (False, 0.0), (False, 0.0), (False, 0.0), (False, 0.0), (False, 0.0), (False, 0.0), (True, 0.0)]\n",
      "\n",
      "True Bag Label, Predicted Bag Label: (True, 1)\n",
      "True Instance Labels, Attention Weights: [(False, 0.26100000739097595), (False, 0.0), (False, 0.0), (True, 0.7360000014305115), (False, 0.0), (True, 0.003000000026077032), (False, 0.0), (False, 0.0)]\n",
      "\n",
      "True Bag Label, Predicted Bag Label: (True, 1)\n",
      "True Instance Labels, Attention Weights: [(False, 0.0010000000474974513), (False, 0.0), (False, 0.04800000041723251), (False, 0.0010000000474974513), (False, 0.0010000000474974513), (False, 0.0020000000949949026), (False, 0.0010000000474974513), (False, 0.0020000000949949026), (False, 0.0010000000474974513), (True, 0.9440000057220459), (False, 0.0010000000474974513)]\n",
      "\n",
      "True Bag Label, Predicted Bag Label: (True, 1)\n",
      "True Instance Labels, Attention Weights: [(False, 0.0020000000949949026), (False, 0.003000000026077032), (False, 0.004000000189989805), (True, 0.19900000095367432), (False, 0.004000000189989805), (False, 0.003000000026077032), (False, 0.7789999842643738), (False, 0.004000000189989805), (False, 0.0020000000949949026)]\n",
      "\n",
      "True Bag Label, Predicted Bag Label: (True, 1)\n",
      "True Instance Labels, Attention Weights: [(False, 0.0), (True, 1.0), (False, 0.0), (False, 0.0), (False, 0.0), (False, 0.0), (False, 0.0), (False, 0.0), (False, 0.0)]\n",
      "\n",
      "Test Set, Loss: 2.5847, Test error: 0.3600\n"
     ]
    }
   ],
   "source": [
    "print('Start Training')\n",
    "for epoch in range(1, args.epochs + 1):\n",
    "    train(epoch)\n",
    "print('Start Testing')\n",
    "test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyPWgGUZuCU4V2amHOGmYmWK",
   "include_colab_link": true,
   "name": "Neural Trees for Multiple Instance Learning.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
